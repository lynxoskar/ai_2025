## this is the work plan for our presentation

## we will have a folder with sources where we download items of interest

## i will dump outlining thoughts here and include into the presentation

## idea - timeline from last presenation 
 - the foundation models    
    previous google bert, meta laba, openai 
    today - many new players


 - still a token predicting engine
    - new resoning 
    - prompt methods
    - temperature, ktop, ptop 
    - echosystem around curating the use of an llm - chain of thougth strategies 

 - is this a real thing. Do we know better today
    - limit of data creating synthetic data, vendors of specific data
    - limit of trainings


 - multimodal 
    - text is perhaps the best area
    - image - to text - then interpret language is compression of knowledge 


- arc - benchmarks 
    https://arcprize.org/
    <blockquote class="twitter-tweet" data-media-max-width="560"><p lang="en" dir="ltr">Yann LeCun: I&#39;m not interested in LLMs anymore - they&#39;re the past. The future is in four more interesting areas: machines that understand the physical world, persistent memory, reasoning, and planning. <a href="https://t.co/qRH7XVlbWq">pic.twitter.com/qRH7XVlbWq</a></p>&mdash; Victor (@victor_explore) <a href="https://twitter.com/victor_explore/status/1910978633000157201?ref_src=twsrc%5Etfw">April 12, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
    https://www.vals.ai/home




